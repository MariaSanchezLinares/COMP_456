
---
title: "Dynamic Topic Modeling on Spanish Newspaper Articles"
author: "Adapted from Bernadeta Griciūtė"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(topicmodels)
library(ldatuning)
library(LDAvis)
library(tidytext)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)


#source script with useful functions

source("./project_func.R")

```

## Introduction

This R Markdown file implements dynamic topic modeling on Spanish newspaper articles using a CSV file. The file contains columns: `date`, `source`, and `content`.

## Data Preparation

```{r data-preparation}
# Load CSV data
file_path <- "./y2023_articles.csv"  # Update with your CSV file path
data <- read.csv(file_path, stringsAsFactors = FALSE)

data <- data %>%
  mutate(date = mdy(date),
         year = year(date),
         month = month(date),
         day = day(date),
         ID = str_pad(X, width = 4, pad = "0")
         ) %>% 
  select(ID, date, title , content, source_name, language, length, nchar, processed_text)

head(data)
```

## De-duplicating data

```{r}

data <- as.data.frame(ea_no_dups(as.data.table(data), opt_key_all = TRUE))

```


## Text Preprocessing

```{r text-preprocessing}
# Create a corpus
corpus <- Corpus(VectorSource(data$processed_text))

# # Convert to lowercase
# corpus <- tm_map(corpus, content_transformer(tolower))
# 
# # Remove punctuation, numbers, and stopwords
# corpus <- tm_map(corpus, removePunctuation)
# corpus <- tm_map(corpus, removeNumbers)
# corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
# 
# # Stem the text (optional)
# library(SnowballC)
# corpus <- tm_map(corpus, stemDocument, language = "spanish")
 
# Convert to plain text
#corpus <- tm_map(corpus, PlainTextDocument)
```

## Tokenization and Term-Document Matrix

```{r tdm-creation}
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)

# Remove sparse terms to optimize computation
dtm <- removeSparseTerms(dtm, 0.99)

# Summary of the DTM
dim(dtm)
```

## Topic Modeling

```{r lda-model}
# Optimal number of topics
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 16, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 1234),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)

```

```{r}

# Fit LDA Model
optimal_topics <- 8  # Update based on results
lda_model <- LDA(dtm, k = optimal_topics, control = list(seed = 1234))

```

```{r}
# Get terms and topics
terms(lda_model, 10)
topics(lda_model)
```

## Visualization

```{r visualization}
# Prepare LDAvis data
library(LDAvis)
library(servr)

json <- createJSON(
  phi = posterior(lda_model)$terms,
  theta = posterior(lda_model)$topics,
  doc.length = rowSums(as.matrix(dtm)),
  vocab = colnames(as.matrix(dtm)),
  term.frequency = colSums(as.matrix(dtm))
)

serVis(json, open.browser = TRUE)
```
### tracing back articles 
```{r}
# # Get topic probabilities for each document
# doc_topic_probs <- documentTopicProb(lda_model)

# # Get the most probable topic for each document (dominant topic)
# dominant_topics <- apply(doc_topic_probs, 1, which.max)
# 
# # Loop through documents and print titles based on dominant topic
# for (i in 1:nrow(data)) {
#   topic <- dominant_topics[i]
#   # Assuming "title" is a column in your data containing article titles
#   title <- data$title[i]
#   cat("Document:", title, " - Dominant Topic:", topic, "\n")
# }
# 
# # Filter articles with a probability greater than a threshold for a specific topic (e.g., topic 3)
# topic_threshold <- 0.7
# topic_of_interest <- 7
# filtered_articles <- data[doc_topic_probs[, topic_of_interest] > topic_threshold, ]
# 
# # Access the "title" column (or any other relevant column) from the filtered data
# filtered_titles <- filtered_articles$title
```

# output table 1

```{r}
# # Extract beta matrix (word-topic probabilities)
# beta_values <- tidy(lda_model, matrix = "beta")  # Extract beta values from LDA model
# 
# # Get the top 20 keywords for each topic
# top_keywords <- beta_values %>%
#   group_by(topic) %>%
#   slice_max(order_by = beta, n = 20) %>%  # Select top 20 words with the highest beta values per topic
#   ungroup() %>%
#   arrange(topic, -beta)
# 
# # Summarize keywords for each topic as a comma-separated string
# table1_keywords <- top_keywords %>%
#   group_by(topic) %>%
#   summarise(keywords = paste(term, collapse = ", ")) %>%
#   ungroup()

```

Identify Representative Headlines:

```{r}
# # Extract gamma matrix (document-topic probabilities)
# gamma_values <- tidy(lda_model, matrix = "gamma")
# 
# # Convert `document` to integer
# gamma_values <- gamma_values %>%
#   mutate(document = as.integer(document))
# 
# # Join with the data to get article details
# top_articles <- gamma_values %>%
#   group_by(topic) %>%
#   slice_max(order_by = gamma, n = 10) %>%  # Select the article with the highest gamma value per topic
#   ungroup() %>%
#   left_join(data, by = c("document" = "X")) %>%  # Join with `X` from the data frame
#   mutate(topic = topic.x) %>% 
#   select(topic, headline = title, date)  # Use 'title' for headlines and 'date' for publication date

```

```{r}
# # Combine keywords and representative headlines into Table 1
# table1 <- table1_keywords %>%
#   left_join(top_articles, by = "topic")
# 
# # View Table 1
# table1

```

```{r}
kable(
  table1,
  caption = "Table 1: Summary of Topics with Top Keywords and Representative Headlines"
)

```

# table 1 parte II

```{r}
# Get the beta values (topic-term probabilities)
beta_values <- tidy(lda_model, matrix = "beta")

# Get the top 20 terms per topic based on beta value
top_terms <- beta_values %>%
  group_by(topic) %>%
  slice_max(order_by = beta, n = 20) %>%  # Select top 20 terms
  arrange(topic, desc(beta)) %>%          # Arrange terms by beta in descending order
  ungroup()

# Create the table with each term and beta value as separate rows
table1 <- top_terms %>%
  select(Topic = topic, Term = term, Beta = beta) %>%  # Rename columns for clarity
  arrange(Topic, desc(Beta))                          # Ensure terms are ordered within each topic

# Print the final table
print(table1)

```



# Gamma values


```{r}

# Function to assign documents to topics based on gamma values
assign_topic <- function(doc_prob, threshold) {
  max_prob <- max(doc_prob)
  if (max_prob > threshold) {
    return(which.max(doc_prob))
  } else {
    return(NA)
  }
}

# Set a threshold (adjust as needed)
threshold <- mean(apply(doc_topic_probs, 1, max))

# Assign topics to documents
topic_assignments <- apply(doc_topic_probs, 1, assign_topic, threshold = threshold)

# Create a data frame with document IDs, topics, and gamma values
topic_df <- data.frame(
  doc_id = 1:nrow(doc_topic_probs),
  topic = topic_assignments,
  max_gamma = apply(doc_topic_probs, 1, max)
)

# Filter out documents with NA topic assignments
topic_df <- topic_df[!is.na(topic_df$topic), ]

# Create a table summarizing topic assignments
topic_table <- table(topic_df$topic)

# Calculate topic prevalence over time (assuming you have a "year" column in your data)
topic_df$year <- data$year  # Replace 'data' with your actual data frame

topic_prevalence <- aggregate(topic ~ year, data = topic_df, FUN = function(x) length(unique(x)))

# Print the table
print(topic_table)

# Plot topic prevalence over time
plot(topic_prevalence, type = "l", xlab = "Year", ylab = "Number of Documents")
```



## Dynamic Topic Modeling (Future Steps)

Incorporate dynamic topic modeling packages or manual partitioning of the data based on `date` to implement a time-series analysis.

---

## Dynamic Topic Modeling

To perform dynamic topic modeling (DTM) using the temporal information in the dataset, follow these steps:

### 1. Data Partitioning

Split the data into subsets based on a time window (e.g., by month or quarter).

```{r data-partitioning}
# Create a time-based grouping (monthly in this example)
data$month <- floor_date(data$date, "month")


# Group the content by month
grouped_data <- data %>%
  mutate(month = month(date)) %>% 
  group_by(month) %>%
  summarise(content = paste(content, collapse = " "))
```

### 2. Generate Time-Sliced Corpora

Create a Document-Term Matrix (DTM) for each time period.

```{r time-sliced-dtm}
# Initialize an empty list to store DTMs
dtm_list <- list()

# Loop through each time period
for (i in 1:nrow(grouped_data)) {
  corpus_time <- Corpus(VectorSource(grouped_data$content[i]))
  corpus_time <- tm_map(corpus_time, content_transformer(tolower))
  corpus_time <- tm_map(corpus_time, removePunctuation)
  corpus_time <- tm_map(corpus_time, removeNumbers)
  corpus_time <- tm_map(corpus_time, removeWords, stopwords("spanish"))
  dtm_time <- DocumentTermMatrix(corpus_time)
  dtm_time <- removeSparseTerms(dtm_time, 0.99)
  dtm_list[[i]] <- dtm_time
}

# Verify the structure
str(dtm_list)
```

### 3. Apply Dynamic Topic Modeling

Use packages like `ldaseq` or external tools for dynamic topic modeling. For example, using the `topicmodels` package:

```{r dtm-analysis}
# Analyze changes in topics over time (manual partition example)
lda_models <- lapply(dtm_list, function(dtm) {
  LDA(dtm, k = optimal_topics, control = list(seed = 1234))
})

# Example: Extract and compare topics for the first two time periods
terms(lda_models[[1]], 10)
terms(lda_models[[2]], 10)
```

### 4. Visualization and Insights

Visualize the evolution of topics over time.

```{r visualize-evolution}
# Example visualization of topic proportions over time
topic_proportions <- lapply(lda_models, function(model) {
  posterior(model)$topics
})

# Combine results into a data frame for visualization
proportions_df <- do.call(rbind, lapply(1:length(topic_proportions), function(i) {
  data.frame(
    Time = grouped_data$month[i],
    Topic = colnames(topic_proportions[[i]]),
    Proportion = rowMeans(topic_proportions[[i]])
  )
}))

ggplot(proportions_df, aes(x = Time, y = Proportion, color = Topic)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Topic Proportions Over Time", x = "Time", y = "Proportion")
```
```{r}

# Table 1: Topic Definitions
topic_terms <- tidy(lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  arrange(topic, -beta) %>%
  group_by(topic) %>%
  summarize(terms = paste(term, collapse = ", "))

print(topic_terms)

# Table 2: Representative Articles
document_topics <- tidy(lda_model, matrix = "gamma") %>%
  group_by(document) %>%
  filter(gamma == max(gamma)) %>%
  ungroup()

data_with_topics <- data %>%
  mutate(doc_id = as.character(row_number())) %>%
  inner_join(document_topics, by = c("doc_id" = "document"))

representative_articles <- data_with_topics %>%
  group_by(topic) %>%
  top_n(5, gamma) %>%
  select(topic, gamma, content) %>%
  arrange(topic, -gamma)

print(representative_articles)

# Table 3: Topic Dynamics Over Time
topic_dynamics <- data_with_topics %>%
  group_by(year, topic) %>%
  summarize(topic_loading = sum(gamma), .groups = "drop") %>%
  pivot_wider(names_from = topic, values_from = topic_loading, values_fill = 0)

print(topic_dynamics)

# Visualization of Topic Dynamics
topic_dynamics_long <- topic_dynamics %>%
  gather(key = "topic", value = "loading", -year)

ggplot(topic_dynamics_long, aes(x = year, y = loading, color = topic)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Topic Dynamics Over Time", x = "Year", y = "Topic Loading")
```



