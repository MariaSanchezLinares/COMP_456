
---
title: "Dynamic Topic Modeling on Spanish Newspaper Articles"
author: "Adapted from Bernadeta Griciūtė"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(topicmodels)
library(ldatuning)
library(LDAvis)
library(tidytext)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(data.table)
library(servr)
library(stringr)  # For str_pad
library(ggthemes)

#source script with useful functions

source("./project_func.R")

```

## Introduction

This R Markdown file implements dynamic topic modeling on Spanish newspaper articles using a CSV file. The file contains columns: `date`, `source`, and `content`.

## Data Preparation

```{r data-preparation}
# Load CSV data
file_path <- "./ALL_articles.csv"  # Update with your CSV file path
data <- read.csv(file_path, colClasses = "character")

head(data)
```

## De-duplicating data

```{r}

data <- as.data.frame(ea_no_dups(as.data.table(data), in_vars_key = c("title")))

```


## Text Preprocessing

```{r text-preprocessing}
# Create a corpus
corpus <- Corpus(VectorSource(data$processed_text))
```

## Tokenization and Term-Document Matrix

```{r tdm-creation}
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)

# Remove sparse terms to optimize computation
dtm <- removeSparseTerms(dtm, 0.99)

# Summary of the DTM
dim(dtm)
```

## Topic Modeling

```{r lda-model}
# # Optimal number of topics
# result <- FindTopicsNumber(
#   dtm,
#   topics = seq(from = 2, to = 16, by = 1),
#   metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 1234),
#   mc.cores = 2L,
#   verbose = TRUE
# )
# 
# FindTopicsNumber_plot(result)

```
-   Griffiths2004: Aims to identify coherence.

-   Deveaud2014: Reflects interpretability.

-   CaoJuan2009: Measures distinctiveness.

-   Arun2010: Analyzes topic separation.

```{r}

# Fit LDA Model
optimal_topics <- 6  # Update based on results
lda_model <- LDA(dtm, k = optimal_topics, control = list(seed = 1234))

```

```{r}
# Get terms and topics
terms(lda_model, 20)
topics(lda_model)
```

Topic number and name xwalk
```{r}
# Create a data frame with topics numbers and topic names
topic_xwalk <- data.frame(
  topic = c(1, 2, 3, 4, 5, 6),
  topic_name = c("Soccer", "Venezuelan Politics", "Colombia-Venezuela Diplomacy", "Migration", "Environmental Degradation", "Guerrilla"),
  stringsAsFactors = FALSE
)

# View the data frame
print(topic_xwalk)


```


## Visualization

```{r visualization}
# Prepare LDAvis data

json <- createJSON(
  phi = posterior(lda_model)$terms,
  theta = posterior(lda_model)$topics,
  doc.length = rowSums(as.matrix(dtm)),
  vocab = colnames(as.matrix(dtm)),
  term.frequency = colSums(as.matrix(dtm))
)

serVis(json, open.browser = TRUE)
```

# Tracing back articles 

## Table 1: keywords with the highest beta value per topic

```{r}
# Get the beta values (topic-term probabilities)
beta_values <- tidy(lda_model, matrix = "beta")

# Get the top 20 terms per topic based on beta value
top_terms <- beta_values %>%
  group_by(topic) %>%
  slice_max(order_by = beta, n = 20) %>%  # Select top 20 terms
  arrange(topic, desc(beta)) %>%          # Arrange terms by beta in descending order
  ungroup()

# Create the table with each term and beta value as separate rows
table1 <- top_terms %>%
  select(topic, term, beta) %>%  
  arrange(topic, desc(beta)) %>%  # Ensure terms are ordered within each topic
  left_join(topic_xwalk, by = "topic")


# Print the final table
print(table1)

```


### Classify articles by topic with Gamma values  

```{r}

# Calculate the document-topic probabilities
doc_topic_probs <- posterior(lda_model)$topics  # Replace with your LDA model object

# Assign unique IDs to each document upfront (IDs remain consistent)
doc_ids <- str_pad(1:nrow(doc_topic_probs), width = 4, pad = "0")

# Calculate thresholds as the mean gamma value for each topic
topic_thresholds <- colMeans(doc_topic_probs)  # A vector with mean gamma value for each topic

# Define a function to assign topics based on gamma values and per-topic thresholds
assign_topic <- function(doc_prob, thresholds) {
  max_prob <- max(doc_prob)
  max_topic <- which.max(doc_prob)
  
  # Check if the maximum gamma value exceeds the threshold for the corresponding topic
  if (max_prob > thresholds[max_topic]) {
    return(max_topic)  # Assign the topic
  } else {
    return(NA)  # Exclude documents not meeting any topic's threshold
  }
}

# Assign topics to documents based on the thresholds
topic_assignments <- apply(doc_topic_probs, 1, assign_topic, thresholds = topic_thresholds)

# Create a data frame with IDs, topics, and maximum gamma values
topic_df <- data.frame(
  ID = doc_ids,                       # Document IDs (already assigned upfront)
  topic = topic_assignments,          # Assigned topics
  max_gamma = apply(doc_topic_probs, 1, max)  # Maximum gamma value for each document
)

# Join the topic assignments (including NA) with the original dataset to retain all documents
merged_data <- topic_df %>%
  left_join(data, by = c("ID" = "ID"))  # Ensure IDs are consistent for merging

# Filter out documents with NA topic assignments (below any topic's threshold)
assigned_articles <- merged_data %>%
  filter(!is.na(topic)) %>%            # Remove unassigned documents
  select(ID, title, topic, max_gamma, everything()) %>%   # Retain relevant columns
  left_join(topic_xwalk, by = "topic") # assign topic name 

assigned_articles
```


#Identify Representative Headlines:

```{r}
# Summarize the top 10 most representative articles for each topic based on gamma values
summary_table <- assigned_articles %>%
  group_by(topic_name) %>%
  arrange(topic_name, desc(max_gamma)) %>%  # Sort by gamma (descending)
  slice_head(n = 5) %>%  # Select top 10 articles for each topic
  select(topic_name, max_gamma, title)   # Keep only the relevant columns

# Print the summary table
print(summary_table)

```

Step 1: Calculate Topic Distribution per Year

```{r}

# Calculate the number and percent of documents assigned to each topic per year
topic_yearly_distribution <- assigned_articles %>%
  group_by(year, topic_name) %>%
  summarise(
    num_docs = n(),
    percent_docs = (num_docs / nrow(data[data$year == unique(year),])) * 100
  ) %>%
  ungroup()

# Print the topic distribution table
print(topic_yearly_distribution)

```

Step 2: Calculate the Number and Percent of Documents Using the "Top 20 Words" per Topic

```{r}
# Assuming 'top_terms' contains the top 20 terms for each topic
top_terms_list <- table1 %>%
  group_by(topic_name) %>%
  summarise(top_words = list(term))

# Define a function to check if a document contains any of the top words
document_contains_top_words <- function(doc_text, top_words) {
  any(sapply(top_words, grepl, x = doc_text, ignore.case = TRUE))
}

# Add a column in assigned_articles to flag documents containing top words
assigned_articles <- assigned_articles %>%
  left_join(top_terms_list, by = "topic_name") %>%
  mutate(contains_top_words = mapply(document_contains_top_words, processed_text, top_words))

# Calculate the number and percent of documents containing top words for each year and topic
top_words_yearly_distribution <- assigned_articles %>%
  group_by(year, topic_name) %>%
  summarise(
    num_docs_with_top_words = sum(contains_top_words),
    percent_docs_with_top_words = (num_docs_with_top_words / n()) * 100
  ) %>%
  ungroup()

# Combine both topic-year distributions in one table if needed
final_table <- left_join(topic_yearly_distribution, top_words_yearly_distribution, by = c("year", "topic_name"))

# Print the final table with both distributions
print(final_table)

```


# LDA within topic 5 

```{r}
topic_5_articles <- assigned_articles %>% 
  filter(topic == 4) %>% 
  select(ID, topic_name, title, date, content, source_name, language, length, nchar, processed_text, year)

# topic_5_articles <- as.data.frame(ea_no_dups(as.data.table(topic_5_articles), in_vars_key = c("title")))

topic_5_articles <- topic_5_articles %>% 
    mutate(ID = str_pad(1:nrow(topic_5_articles), width = 4, pad = "0"))  

```

## Text Preprocessing topic 5

```{r text-preprocessing}
# Create a corpus
corpus5 <- Corpus(VectorSource(topic_5_articles$processed_text))

```

## Tokenization and Term-Document Matrix topic 5 

```{r tdm-creation}
# Create a Document-Term Matrix (DTM)
dtm5 <- DocumentTermMatrix(corpus5)

# Remove sparse terms to optimize computation
dtm5 <- removeSparseTerms(dtm5, 0.99)

# Summary of the DTM
dim(dtm5)
```

```{r}
# # Optimal number of topics
# result5 <- FindTopicsNumber(
#   dtm5,
#   topics = seq(from = 2, to = 16, by = 1),
#   metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 1234),
#   mc.cores = 2L,
#   verbose = TRUE
# )
# 
# FindTopicsNumber_plot(result5)

```

```{r}
# Fit LDA Model
optimal_topics5 <- 8  # Update based on results
lda_model5 <- LDA(dtm5, k = optimal_topics5, control = list(seed = 1234))

```

```{r}
# Get terms and topics
terms(lda_model5, 20)
#topics(lda_model5)
```

```{r}
topic5_xwalk <- data.frame(
  topic = c(1, 2, 3, 4, 5, 6, 7, 8),
  topic_name = c("Gaza", "Colombia-Venezuelan Border Security", "Darien Migration Route", "Venezuela Migration Crisis", "COVID-19", "Health", "Migrant Human Trafficking", "Venezuelan Migrants-US"),
  stringsAsFactors = FALSE
)

head(topic5_xwalk)
```


## Table 1: keywords with the highest beta value per topic

```{r}
# Get the beta values (topic-term probabilities)
beta_values5 <- tidy(lda_model5, matrix = "beta")

# Get the top 20 terms per topic based on beta value
top_terms5 <- beta_values5 %>%
  group_by(topic) %>%
  slice_max(order_by = beta, n = 20) %>%  # Select top 20 terms
  arrange(topic, desc(beta)) %>%          # Arrange terms by beta in descending order
  ungroup() 


# Create the table with each term and beta value as separate rows
table1_5 <- top_terms5 %>%
  left_join(topic5_xwalk, by = "topic") %>% 
  select(topic_name, term, beta) %>%  
  arrange(topic_name, desc(beta))  # Ensure terms are ordered within each topic


# Print the final table
print(table1_5)
```


# Assign topic 5 articles

```{r}
# Calculate the document-topic probabilities
doc_topic_probs5 <- posterior(lda_model5)$topics  # Replace with your LDA model object

# Assign unique IDs to each document upfront (IDs remain consistent)
doc_ids5 <- str_pad(1:nrow(doc_topic_probs5), width = 4, pad = "0")

# Calculate thresholds as the mean gamma value for each topic
topic_thresholds5 <- colMeans(doc_topic_probs5)  # A vector with mean gamma value for each topic

# Define a function to assign topics based on gamma values and per-topic thresholds
assign_topic5 <- function(doc_prob, thresholds) {
  max_prob <- max(doc_prob)
  max_topic <- which.max(doc_prob)
  
  # Check if the maximum gamma value exceeds the threshold for the corresponding topic
  if (max_prob > thresholds[max_topic]) {
    return(max_topic)  # Assign the topic
  } else {
    return(NA)  # Exclude documents not meeting any topic's threshold
  }
}

# Assign topics to documents based on the thresholds
topic_assignments5 <- apply(doc_topic_probs5, 1, assign_topic5, thresholds = topic_thresholds5)

# Create a data frame with IDs, topics, and maximum gamma values
topic_df5 <- data.frame(
  ID = doc_ids5,                       # Document IDs (already assigned upfront)
  topic = topic_assignments5,          # Assigned topics
  max_gamma = apply(doc_topic_probs5, 1, max)  # Maximum gamma value for each document
)

# Join the topic assignments (including NA) with the original dataset to retain all documents
merged_data5 <- topic_df5 %>%
  left_join(topic_5_articles, by = c("ID" = "ID"))  # Ensure IDs are consistent for merging

# Filter out documents with NA topic assignments (below any topic's threshold)
assigned_articles5 <- merged_data5 %>%
  filter(!is.na(topic)) %>%            # Remove unassigned documents
  select(ID, title, topic, max_gamma, date, content, source_name, language, length, nchar, processed_text, year) %>%  # Retain relevant columns
  left_join(topic5_xwalk, by = "topic") # assign topic name 


assigned_articles5
```


#Identify Representative Headlines:

```{r}
# Summarize the top 10 most representative articles for each topic based on gamma values
summary_table5 <- assigned_articles5 %>%
  group_by(topic_name) %>%
  arrange(topic_name, desc(max_gamma)) %>%  # Sort by gamma (descending)
  slice_head(n = 5) %>%  # Select top 10 articles for each topic
  select(topic_name, max_gamma, title)  # Keep only the relevant columns

# Print the summary table
print(summary_table5)

```

Step 1: Calculate Topic Distribution per Year

```{r}

# Calculate the number and percent of documents assigned to each topic per year
topic_yearly_distribution5 <- assigned_articles5 %>%
  group_by(year, topic_name) %>%
  summarise(
    num_docs = n(),
    percent_docs = (num_docs / nrow(topic_5_articles[topic_5_articles$year == unique(year),])) * 100
  ) %>%
  ungroup()

# Print the topic distribution table
print(topic_yearly_distribution5)

```

Step 2: Calculate the Number and Percent of Documents Using the "Top 20 Words" per Topic

```{r}
# Assuming 'top_terms' contains the top 20 terms for each topic
top_terms_list5 <- table1_5 %>%
  group_by(topic_name) %>%
  summarise(top_words = list(term))

# Add a column in assigned_articles to flag documents containing top words
assigned_articles5 <- assigned_articles5 %>%
  left_join(top_terms_list5, by = "topic_name") %>%
  mutate(contains_top_words = mapply(document_contains_top_words, processed_text, top_words))

# Calculate the number and percent of documents containing top words for each year and topic
top_words_yearly_distribution5 <- assigned_articles5 %>%
  group_by(year, topic_name) %>%
  summarise(
    num_docs_with_top_words = sum(contains_top_words),
    percent_docs_with_top_words = (num_docs_with_top_words / n()) * 100
  ) %>%
  ungroup()

# Combine both topic-year distributions in one table if needed
final_table5 <- left_join(topic_yearly_distribution5, top_words_yearly_distribution5, by = c("year", "topic_name"))

# Print the final table with both distributions
print(final_table5)

```

#Visualization

Topic Distribution per Year
```{r}
ggplot(topic_yearly_distribution, aes(x = year, y = percent_docs, fill = as.factor(topic_name))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Topic Distribution per Year (Main LDA)", 
       x = "Year", 
       y = "Percentage of Documents", 
       fill = "Topic") +
  theme_minimal()

```

