```{r}
# 1. Load Necessary Libraries
library(dplyr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(lubridate)      # For date manipulation functions
library(tidytext)       # For tokenization
library(pdftools) # For PDF text extraction
library(stringr)    # For string manipulation functions, including str_extract()
library(SnowballC)  # For the wordStem function

# 2. Define Paths and Load PDF Content
pdf_path <- "webscrapping /pdf/350.000 venezolanos radicados en el país en los últimos 6 años.PDF"
pdf_dir <- "webscrapping /pdf/"

# Load the text of the first page to check content
pdf_text_data <- pdf_text(pdf_path)
cat(pdf_text_data[1])

# 3. Define Custom Stop Words and Preprocessing Function
custom_stopwords <- c(stopwords("es"), "las", "los", "que", "y", "han")

# Define a text preprocessing function
preprocess_text <- function(text) {
  text <- tolower(text)
  text <- str_replace_all(text, "[^[:alpha:][:space:]]", " ")
  text <- str_replace_all(text, "\\s+", " ")
  text <- str_trim(text)
  tokens <- unlist(strsplit(text, "\\s+"))
  tokens <- tokens[!tokens %in% custom_stopwords]
  tokens <- wordStem(tokens, language = "spanish")
  paste(tokens, collapse = " ")
}

# 4. Process All PDFs to Extract Article Data
pdf_files <- list.files(pdf_dir, pattern = "\\.PDF$", full.names = TRUE)
articles_list <- list()

for (file in pdf_files) {
  pdf_text_data <- pdf_text(file)
  pdf_text <- paste(pdf_text_data, collapse = " ")
  title <- tools::file_path_sans_ext(basename(file))
  date <- str_extract(pdf_text, "\\d{1,2} \\w+ \\d{4}")
  date <- ifelse(!is.na(date), date, "Date not found")
  article_body <- str_remove(pdf_text, paste0("Page\\s*1\\s*of\\s*2\\s*", title, ".*"))
  article_body <- str_replace(article_body, "(?i)(Copyright.*|Classification|Language:|Load-Date:.*$)", "")
  newspaper <- str_extract(pdf_text, "(?i)\\b(by|de|from|en)\\s+([A-Za-z\\s]+?)(?=\\s(?:\\(|\\d|\\s|Content|Copyright))")
  newspaper <- ifelse(!is.na(newspaper), newspaper, "Newspaper not found")
  trimmed_article_body <- substr(article_body, 876, nchar(article_body) - 475)
  cleaned_article_body <- preprocess_text(trimmed_article_body)

  articles_list[[file]] <- data.frame(
    title = title,
    date = date,
    original_body = trimmed_article_body,
    body = cleaned_article_body,
    stringsAsFactors = FALSE
  )
}

# Combine Data Frames into One
articles_data <- do.call(rbind, articles_list)

# 5. Define Date Conversion Function for Spanish Dates
convert_date <- function(date_str) {
  months <- c("enero" = 1, "febrero" = 2, "marzo" = 3,
              "abril" = 4, "mayo" = 5, "junio" = 6,
              "julio" = 7, "agosto" = 8, "septiembre" = 9,
              "octubre" = 10, "noviembre" = 11, "diciembre" = 12)
  date_parts <- unlist(strsplit(date_str, " "))
  day <- as.integer(date_parts[1])
  month <- months[date_parts[2]]
  year <- as.integer(date_parts[3])
  if (is.na(month)) { return(NA) }
  return(as.Date(paste(year, month, day, sep = "-")))
}

# 6. Data Cleaning and Filtering
articles_data <- articles_data %>%
  mutate(
    date = sapply(date, convert_date),  # Apply date conversion
    day = day(as.Date(date)),           # Extract day
    month = month(as.Date(date)),       # Extract month
    year = year(as.Date(date))          # Extract year
  ) %>%
  filter(!is.na(body) & trimws(body) != "")

# 7. Group Articles by Year for Word Cloud Creation
articles_grouped <- articles_data %>%
  group_by(year) %>%
  summarise(text = paste(body, collapse = " "))

# Function to Create a Word Cloud from Text
create_word_cloud <- function(text_data, year) {
  corpus <- Corpus(VectorSource(text_data))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, custom_stopwords)
  corpus <- tm_map(corpus, stripWhitespace)
  tdm <- TermDocumentMatrix(corpus)
  tdm_matrix <- as.matrix(tdm)
  word_freqs <- sort(rowSums(tdm_matrix), decreasing = TRUE)
  word_freqs_df <- data.frame(word = names(word_freqs), freq = word_freqs)
  
  wordcloud(words = word_freqs_df$word,
            freq = word_freqs_df$freq,
            min.freq = 2,
            max.words = 100,
            random.order = FALSE,
            rot.per = 0.35,
            colors = brewer.pal(8, "Dark2"),
            main = paste("Word Cloud for Year", year))
}

# Generate Word Clouds for Each Year
for (i in 1:nrow(articles_grouped)) {
  create_word_cloud(articles_grouped$text[i], articles_grouped$year[i])
}

# 8. Generate and Filter Bigrams and Trigrams
bigrams <- articles_data %>%
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>%
  filter(!bigram %in% c("pag of","copyright grup", "gda tiemp", "grup diari", "colombi derech")) %>%
  count(bigram, sort = TRUE)

trigrams <- articles_data %>%
  unnest_tokens(trigram, body, token = "ngrams", n = 3) %>%
  filter(!trigram %in% c("copyright grup diari", "gda tiemp colombi", "tiemp colombi derech", 
                         "colombi derech reserv", "grup diari amer", "diari amer gda", 
                         "derech reserv prohib","prohib uso reproduccion", 
                         "amer gda tiemp","reserv prohib uso", "uso reproduccion colombi")) %>%
  count(trigram, sort = TRUE)

# Display Counts
print(bigrams)
print(trigrams)
```

