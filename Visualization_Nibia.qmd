---
title: "Visualization"
format: html
editor: visual
---

### Packages

```{r, message = FALSE}
# Load Necessary Libraries
library(dplyr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(lubridate)      # For date manipulation functions
library(tidytext)       # For tokenization
library(pdftools) # For PDF text extraction
library(stringr)    # For string manipulation functions, including str_extract()
library(SnowballC)  # For the wordStem function
library(ggplot2)
library(textstem)
library(pdftools)
library(stringr)
library(purrr)
library(fs)  # For file handling
library(dplyr)
library(widyr)
library(igraph)
library(ggraph)

```

```{r}
# maria's path
pdf_text_data <- pdf_text("./2023_el_tiempo/Advertencia de EE. UU. a Venezuela _ 'sanciones volverán si no hay progreso pronto'.PDF")


print(pdf_text_data[1]) #For page 1
```

Now try this:

```{r}
cat(pdf_text_data[1]) #For page 1
```

### Defining the function

```{r}

# Function to extract relevant information from a single PDF
extract_pdf_info <- function(pdf_file) {
  # Extract text from all pages of the PDF and concatenate
  pdf_text_data <- pdf_text(pdf_file)
  full_text <- paste(pdf_text_data, collapse = "\n")  # Combine all pages

  # Define extraction functions
  extract_date <- function(text) {
    date_pattern <- "(?<=Load-Date:)\\s*\\w+ \\d{1,2}, \\d{4}"
    date <- str_extract(text, date_pattern)
    return(date)
  }
  
  extract_title <- function(text) {
    lines <- str_split(text, "\n")[[1]]
    title <- lines[2]  # The title is on the second line
    return(str_trim(title))
  }

  extract_main_text <- function(text) {
    body_start <- str_locate(text, "Body")[, 1]
    classification_start <- str_locate(text, "Classification")[, 1]
    end_document_start <- str_locate(text, "End of Document")[, 1]
    main_text_end <- min(c(na.omit(c(classification_start, end_document_start))), na.rm = TRUE)
    
    if (!is.na(body_start) && !is.na(main_text_end)) {
      main_text <- str_sub(text, body_start + 5, main_text_end - 1)  # +5 to skip "Body"
    } else {
      main_text <- text
    }
    return(str_trim(main_text))
  }

  extract_newspaper <- function(text) {
    newspaper_pattern <- "(?<=Byline:)\\s*[^\\n]+"
    newspaper <- str_extract(text, newspaper_pattern)
    return(str_trim(newspaper))
  }

  extract_length <- function(text) {
    length_pattern <- "(?<=Length:)\\s*\\d+ words"
    length <- str_extract(text, length_pattern)
    return(str_trim(length))
  }

  extract_language <- function(text) {
    language_pattern <- "(?<=Language:)\\s*\\w+"
    language <- str_extract(text, language_pattern)
    return(language)
  }

  # Apply extraction functions on the combined text from all pages
  date <- extract_date(full_text)
  title <- extract_title(full_text)
  main_text <- extract_main_text(full_text)
  newspaper <- extract_newspaper(full_text)
  length <- extract_length(full_text)
  language <- extract_language(full_text)

  # Create a data frame for this PDF
  pdf_df <- data.frame(
    date = date,
    title = title,
    content = main_text,
    source_name = newspaper,
    language = language,
    length = length,
    stringsAsFactors = FALSE
  )
  return(pdf_df)
}

```

## Try on one pdf

```{r}
pdf_1 <- extract_pdf_info("./2023_el_tiempo/Advertencia de EE. UU. a Venezuela _ 'sanciones volverán si no hay progreso pronto'.PDF")

print(pdf_1)
```

## Loading El Tiempo corpus data

```{r}
# Define PDF directory
el_tiempo_pdf_directory <- "./2023_el_tiempo/"

# Get a list of all PDF files in the directory
el_tiempo_pdf_files <- dir_ls(el_tiempo_pdf_directory, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_tiempo_pdf_df_2023 <- map_df(el_tiempo_pdf_files, extract_pdf_info)

```

```{r}
# Define PDF directory
el_espectador_pdf_directory <- "./2023_el_espectador/"

# Get a list of all PDF files in the directory
el_espectador_pdf_files <- dir_ls(el_espectador_pdf_directory, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_espectador_pdf_df_2023<- map_df(el_espectador_pdf_files, extract_pdf_info)

```

### el tiempo 2017

```{r}
# Define PDF directory
el_tiempo_pdf_directory_2017 <- "./2017_el_tiempo/"

# Get a list of all PDF files in the directory
el_tiempo_pdf_files_2017 <- dir_ls(el_tiempo_pdf_directory_2017, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_tiempo_pdf_df_2017 <- map_df(el_tiempo_pdf_files_2017, extract_pdf_info)

```

### el espectador 2017

```{r}
# Define PDF directory
el_espectador_pdf_directory_2017 <- "./2017_el_espectador/"

# Get a list of all PDF files in the directory
el_espectador_pdf_files_2017 <- dir_ls(el_espectador_pdf_directory_2017, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_espectador_pdf_df_2017 <- map_df(el_espectador_pdf_files_2017, extract_pdf_info)

```

### el espectador 2020

```{r}
# Define PDF directory
el_espectador_pdf_directory_2020 <- "./2020_el_espectador/"

# Get a list of all PDF files in the directory
el_espectador_pdf_files_2020 <- dir_ls(el_espectador_pdf_directory_2020, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_espectador_pdf_df_2020 <- map_df(el_espectador_pdf_files_2020, extract_pdf_info)

```

### el tiempo 2020

```{r}
# Define PDF directory
el_tiempo_pdf_directory_2020 <- "./2020_el_tiempo/"

# Get a list of all PDF files in the directory
el_tiempo_pdf_files_2020 <- dir_ls(el_tiempo_pdf_directory_2020, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_tiempo_pdf_df_2020 <- map_df(el_tiempo_pdf_files_2020, extract_pdf_info)

```

## append datasets and clean data

```{r}
y2017_articles <- rbind(el_espectador_pdf_df_2017, el_tiempo_pdf_df_2017)

y2020_articles <- rbind(el_tiempo_pdf_df_2020, el_espectador_pdf_df_2020)

y2023_articles <- rbind(el_espectador_pdf_df_2023, el_tiempo_pdf_df_2023)

# 5. Define Date Conversion Function for Spanish Dates
convert_date <- function(date_str) {
  months <- c("enero" = 1, "febrero" = 2, "marzo" = 3,
              "abril" = 4, "mayo" = 5, "junio" = 6,
              "julio" = 7, "agosto" = 8, "septiembre" = 9,
              "octubre" = 10, "noviembre" = 11, "diciembre" = 12)
  date_parts <- unlist(strsplit(date_str, " "))
  day <- as.integer(date_parts[1])
  month <- months[date_parts[2]]
  year <- as.integer(date_parts[3])
  if (is.na(month)) { return(NA) }
  return(as.Date(paste(year, month, day, sep = "-")))
}

all_articles <- y2023_articles %>%
  mutate(date = mdy(date),
         year = year(date),
         month = month(date),
         day = day(date))

```

### Text pre-processing

```{r}
# Define a text pre-processing function
stop_words <- c(stopwords("es"), "las", "los", "que", "y", "han")  # Define stop words once

# Preprocessing function
preprocess_text <- function(text) {
  text <- tolower(text)  # Convert to lowercase
  text <- str_replace_all(text, "[^[:alpha:][:space:]]", " ")  # Remove punctuation/special characters
  text <- str_replace_all(text, "\\s+", " ")  # Replace multiple spaces with a single space
  text <- str_trim(text)  # Trim leading and trailing spaces
  tokens <- unlist(strsplit(text, "\\s+"))  # Split text into words
  tokens <- tokens[!tokens %in% stop_words]  # Remove stop words
  tokens <- lemmatize_words(tokens)  # Lemmatize the words
  paste(tokens, collapse = " ")  # Combine tokens back into a single string
}

# Apply preprocessing to each article’s content
all_articles_processed <- all_articles %>%
  mutate(processed_content = map_chr(content, preprocess_text))

# View the processed data
head(all_articles_processed)

```

### Tokenization

```{r}
library(tidytext)

# Tokenize and get total word count
tidy_text <- all_articles_processed %>%
  unnest_tokens(word, processed_content) %>%  # Tokenize each word
  filter(!word %in% stop_words)  # Filter out stop words

# Aggregate word counts across all documents
total_word_counts <- tidy_text %>%
  count(word, sort = TRUE)

# View the total word counts
print(total_word_counts)

```

### Calculating TF-IDF

```{r}
# Calculate TF-IDF per document
tf_idf <- tidy_text %>%
  count(title, word, sort = TRUE) %>%  # Count occurrences of each word per document
  bind_tf_idf(word, title, n)  # Calculate tf-idf for each term in each document

# Step 3: Display Top Terms by TF-IDF
top_tf_idf <- tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 10) %>%  # Select top 10 terms per document
  ungroup()

top_tf_idf %>%
  arrange(desc(tf_idf)) %>% 
  print()

# # Step 4: Visualize Top TF-IDF Terms
# ggplot(top_tf_idf, aes(x = reorder_within(word, tf_idf, title), y = tf_idf, fill = title)) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~ title, scales = "free_y") +
#   scale_x_reordered() +
#   labs(x = "Words", y = "TF-IDF") +
#   coord_flip()l;

```

```{r}

# Step 1: Tokenize the text content for each article
tidy_text <- all_articles_processed %>%
  unnest_tokens(word, processed_content) %>%  # Tokenize the `processed_content` column
  filter(!word %in% stop_words)  # Remove stop words
```

```{r}
# Look at most frequent terms co-occurring with "inmigrantes"
tidy_text %>%
  filter(word == "venezuela")

tidy_text %>%
  group_by(word) %>% 
  count()
```

```{r}
# Step 2: Calculate co-occurrence counts for pairs of words in each document
# This creates pairs of words that appear together in the same document
word_pairs <- tidy_text %>%
  pairwise_count(word, title, sort = TRUE) %>%
  filter(item1 == "venezuela" | item2 == "venezuela")  # Keep pairs that include "inmigrantes"

# Step 3: Create the graph
# Rename columns for compatibility with igraph
colnames(word_pairs) <- c("word1", "word2", "n")
graph <- graph_from_data_frame(word_pairs, directed = FALSE)

# Step 4: Visualize the Network
# Customize node size and edge transparency by co-occurrence count
ggraph(graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), color = 'blue3') +  # Edge transparency by co-occurrence count
  geom_node_point(size = 1, color = "skyblue") +  # Nodes representing words
  geom_node_text(aes(label = name), repel = TRUE, size = 4) +  # Word labels with repelling to avoid overlap
  theme_void() +
  labs(title = "Connections of 'Venezuela' in Colombian Newspapers",
       subtitle = "Network of words frequently co-occurring with 'Venezuela'") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


```

```{r}
# Define words to exclude
excluded_words <- c("page", "of", "tiempo", "uso")

# Filter out rows where either word1 or word2 is in the excluded words list
top_cooccurrences <- word_pairs %>%
  filter((word1 == "venezuela" | word2 == "venezuela") & 
         !word1 %in% excluded_words & 
         !word2 %in% excluded_words) %>%
  arrange(desc(n)) %>%
  head(30)  # Display top 10 co-occurring words

# Display the table
print(top_cooccurrences)

```

```{r}
# Define words to exclude
excluded_words <- c("page", "of", "tiempo", "uso")

# Step 1: Filter out specified words and ensure "venezuela" is present
filtered_pairs <- word_pairs %>%
  filter((word1 == "venezuela" | word2 == "venezuela") & 
         !word1 %in% excluded_words & 
         !word2 %in% excluded_words)

# Step 2: Make pairs unique by sorting words alphabetically within each pair
unique_pairs <- filtered_pairs %>%
  mutate(word1 = pmin(word1, word2),  # Ensure word1 is alphabetically first
         word2 = pmax(word1, word2)) %>%  # Ensure word2 is alphabetically second
  distinct(word1, word2, .keep_all = TRUE)  # Remove duplicate pairs

# Step 3: Display the top 10 unique co-occurrences
top_cooccurrences <- unique_pairs %>%
  arrange(desc(n)) %>%
  head(10)  # Show top 10 pairs

# Display the table
print(top_cooccurrences)

```

```{r}
# Define words to exclude
excluded_words <- c("page", "of", "tiempo", "uso", "copyright", "dijo", "según", "dos", "ser", "si", "solo", "además", "parte", " puede", "tras", "aunque", "hoy", "pues", "luego", "mientras", "hacer", "full", "text", "hace", "ayer", "año", "años", "abstract","así", "después", "aseguró", "semana", "tres", "embargo", "mismo", "puede", "vez", "va", "menos", "reproducción", "cada", "die", "deben", "debe", "anunció", "domingo", "jueves", "hacia")

# Step 1: Filter out specified words and ensure "venezuela" is present
filtered_pairs <- word_pairs %>%
  filter((word1 == "venezuela" | word2 == "venezuela") & 
         !word1 %in% excluded_words & 
         !word2 %in% excluded_words)

# Step 2: Make pairs unique by sorting words alphabetically within each pair
# and ensure word1 and word2 are not the same
unique_pairs <- filtered_pairs %>%
  mutate(word1 = pmin(word1, word2),  # Ensure word1 is alphabetically first
         word2 = pmax(word1, word2)) %>%  # Ensure word2 is alphabetically second
  filter(word1 != word2) %>%  # Remove pairs where word1 is the same as word2
  distinct(word1, word2, .keep_all = TRUE)  # Remove duplicate pairs

# Step 3: Display the top 10 unique co-occurrences
top_cooccurrences <- unique_pairs %>%
  arrange(desc(n)) %>%
  head(50)  
# Display the table
print(top_cooccurrences)

top_cooccurrences %>%
  ggplot(aes(x = reorder(word1, n), y = n)) + 
  geom_col() +
  coord_flip()+
  labs(title = "Top Co-occurring Words with 'Venezuela' 2023",
       x = "Word",
       y = "Co-occurrence Count") +
  theme_classic()

```

```{r}
# Define quantile thresholds for high, medium, and low connection strengths
high_threshold <- quantile(unique_pairs$n, 0.66)
medium_threshold <- quantile(unique_pairs$n, 0.33)

# Split into high, medium, and low based on thresholds
high_pairs <- unique_pairs %>% filter(n >= high_threshold)
medium_pairs <- unique_pairs %>% filter(n < high_threshold & n >= medium_threshold)
low_pairs <- unique_pairs %>% filter(n < medium_threshold)

```

```{r}
# High connection graph
high_graph <- graph_from_data_frame(high_pairs, directed = FALSE)

ggraph(high_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), color = 'blue3', edge_width = 1.5) +  # Thicker edges for high connections
  geom_node_point(size = 4, color = "skyblue") +  # Larger nodes for high connections
  geom_node_text(aes(label = name), repel = TRUE, size = 4) +
  theme_void() +
  labs(title = "High Co-occurrence Connections of 'Venezuela'",
       subtitle = "Top words with strong co-occurrence with 'Venezuela'") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```
