---
title: "01_text_mining"
format: html
editor: visual
---

### Packages

```{r, message = FALSE}
# Load Necessary Libraries
library(dplyr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(lubridate)      # For date manipulation functions
library(tidytext)       # For tokenization
library(pdftools) # For PDF text extraction
library(stringr)    # For string manipulation functions, including str_extract()
library(SnowballC)  # For the wordStem function
library(ggplot2)
library(textstem)
library(pdftools)
library(stringr)
library(purrr)
library(fs)  # For file handling

```

```{r}
pdf_text_data <- pdf_text("~/Desktop/STATS 456/Learning Guide/Text_Retrieval_from_PDFs/Barbie Movie 2/Barbie movie merchandise_ From Barbie dolls to a Zara collaboration.pdf")

print(pdf_text_data[1]) #For page 1
```

Now try this:

```{r}
cat(pdf_text_data[1]) #For page 1
```

### Defining the function

```{r}
# Define PDF directory
barbie_pdf_directory <- "~/Desktop/STATS 456/Learning Guide/Barbie Movie 2/"

# Function to extract date, title, and main text from a single PDF
extract_pdf_info <- function(pdf_file) {
  # Extract text from all pages of the PDF and concatenate
  pdf_text_data <- pdf_text(pdf_file)
  full_text <- paste(pdf_text_data, collapse = "\n")  # Combine all pages
  
  # Define extraction functions
  extract_date <- function(text) {
    date_pattern <- "\\b\\w+ \\d{1,2}, \\d{4}\\b"  # Pattern for 'Month Day, Year'
    date <- str_extract(text, date_pattern)
    return(date)
  }
  
  extract_title <- function(text) {
    lines <- str_split(text, "\n")[[1]]
    title <- lines[2]  # The title is on the second line
    return(str_trim(title))
  }
  
  extract_main_text <- function(text) {
    # Locate the start of the main text after "Body"
    body_start <- str_locate(text, "Body")[, 1]
    
    # Locate the end of the main text before "Classification" or "End of Document"
    classification_start <- str_locate(text, "Classification")[, 1]
    end_document_start <- str_locate(text, "End of Document")[, 1]
  
    # Determine the ending point of the main text
    main_text_end <- min(c(na.omit(c(classification_start, end_document_start))), na.rm = TRUE)
  
    # Extract main text if "Body" and end marker are found
    if (!is.na(body_start) && !is.na(main_text_end)) {
      main_text <- str_sub(text, body_start + 5, main_text_end - 1)  # +5 to skip "Body"
      } else {
        main_text <- text  # Fallback to the entire text if markers are missing
        }
    return(str_trim(main_text))
  }
  
  # Function to extract the newspaper name
  extract_newspaper <- function(text) {
    newspaper_pattern <- "\\bThe Independent\\b"
    newspaper <- str_extract(text, newspaper_pattern)
    return(newspaper)
  }
  
  # Function to extract the highlight
  extract_highlight <- function(text) {
    highlight_pattern <- "(?<=Highlight:)\\s*(.*?)\\s*(?=Body)"
    highlight <- str_extract(text, highlight_pattern)
    return(str_trim(highlight))
  }
  
  # Function to extract the language
  extract_language <- function(text) {
    language_pattern <- "(?<=Language:)\\s*(\\w+)"
    language <- str_extract(text, language_pattern)
    return(language)
  }
  
  # Function to extract the publication type
  extract_publication_type <- function(text) {
    publication_type_pattern <- "(?<=Publication-Type:)\\s*(\\w+\\s*\\w*)"
    publication_type <- str_extract(text, publication_type_pattern)
    return(publication_type)
  }
  
  # Function to extract the author
  extract_author <- function(text) {
    author_pattern <- "(?<=Byline:)\\s*(.*?)\\s*(?=Highlight)"
    author <- str_extract(text, author_pattern)
    return(str_trim(author))
  }
  
  # Function to extract the section
  extract_section <- function(text) {
    section_pattern <- "(?<=Section:)\\s*(.*?)\\s*(?=Length)"
    section <- str_extract(text, section_pattern)
    return(str_trim(section))
  }

  # Apply extraction functions on the combined text from all pages
  date <- extract_date(full_text)
  title <- extract_title(full_text)
  main_text <- extract_main_text(full_text)
  newspaper <- extract_newspaper(full_text)
  highlight <- extract_highlight(full_text)
  language <- extract_language(full_text)
  publication_type <- extract_publication_type(full_text)
  author <- extract_author(full_text)
  section <- extract_section(full_text)
  
  # Create a data frame for this PDF
  pdf_df <- data.frame(
    date = date,
    title = title,
    content = main_text,
    source_name = newspaper,
    laguage = language,
    highlight = highlight,
    publication_type = publication_type,
    author = author,
    section = section,
    stringsAsFactors = FALSE
  )
  
  return(pdf_df)
}



```

## Loading Barbie Movie corpus data

```{r}
# Get a list of all PDF files in the directory
barbie_pdf_files <- dir_ls(barbie_pdf_directory, glob = "*.pdf")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
barbie_pdf_df <- map_df(barbie_pdf_files, extract_pdf_info)

# Display the combined data frame
print(barbie_pdf_df)
```

### Text pre-processing

```{r}
# Define a text pre-processing function
stop_words <- stopwords("en")  # Define stop words once

preprocess_text <- function(text) {
  text <- tolower(text)  # Convert to lowercase
  text <- str_replace_all(text, "[^[:alpha:][:space:]]", " ")  # Remove punctuation/special characters
  text <- str_replace_all(text, "\\s+", " ")  # Replace multiple spaces with a single space
  text <- str_trim(text)  # Trim leading and trailing spaces
  tokens <- unlist(strsplit(text, "\\s+"))  # Split text into words
  tokens <- tokens[!tokens %in% stop_words]  # Remove English stop words
  tokens <- lemmatize_words(tokens)  # Lemmatize the words
  paste(tokens, collapse = " ")  # Combine tokens back into a single string
}

# Apply preprocessing to each articleâ€™s content
barbie_pdf_df <- barbie_pdf_df %>%
  mutate(processed_content = map_chr(content, preprocess_text))

# View the processed data
head(barbie_pdf_df)

```

### Tokenization

```{r}
# Step 1: Tokenize and Preprocess Text for TF-IDF
tidy_text <- barbie_pdf_df %>%
  unnest_tokens(word, processed_content) %>%  # Tokenize the `processed_content` column
  filter(!word %in% stop_words)  # Remove stop words
```

### Calculating TF-IDF

```{r}
# Step 2: Calculate TF-IDF
tf_idf <- tidy_text %>%
  count(title, word, sort = TRUE) %>%  # Count occurrences of each word per document
  bind_tf_idf(word, title, n)  # Calculate tf-idf for each term in each document

# Step 3: Display Top Terms by TF-IDF
top_tf_idf <- tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 10) %>%  # Select top 10 terms per document
  ungroup()

# Step 4: Visualize Top TF-IDF Terms
ggplot(top_tf_idf, aes(x = reorder_within(word, tf_idf, title), y = tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ title, scales = "free_y") +
  scale_x_reordered() +
  labs(x = "Words", y = "TF-IDF") +
  coord_flip()

```