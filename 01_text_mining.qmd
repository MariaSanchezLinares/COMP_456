---
title: "01_text_mining"
format: html
editor: visual
---

### Packages

```{r, message = FALSE}
# Load Necessary Libraries
library(dplyr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(lubridate)      # For date manipulation functions
library(tidytext)       # For tokenization
library(pdftools) # For PDF text extraction
library(stringr)    # For string manipulation functions, including str_extract()
library(SnowballC)  # For the wordStem function
library(ggplot2)
library(textstem)
library(pdftools)
library(stringr)
library(purrr)
library(fs)  # For file handling

```

```{r}
pdf_text_data <- pdf_text("~/Desktop/MEN Project/2023_el_tiempo/Advertencia de EE. UU. a Venezuela _ 'sanciones volverán si no hay progreso pronto'.PDF")

print(pdf_text_data[1]) #For page 1
```

Now try this:

```{r}
cat(pdf_text_data[1]) #For page 1
```

### Defining the function

```{r}

# Function to extract relevant information from a single PDF
extract_pdf_info <- function(pdf_file) {
  # Extract text from all pages of the PDF and concatenate
  pdf_text_data <- pdf_text(pdf_file)
  full_text <- paste(pdf_text_data, collapse = "\n")  # Combine all pages

  # Define extraction functions
  extract_date <- function(text) {
    date_pattern <- "(?<=Load-Date:)\\s*\\w+ \\d{1,2}, \\d{4}"
    date <- str_extract(text, date_pattern)
    return(date)
  }
  
  extract_title <- function(text) {
    lines <- str_split(text, "\n")[[1]]
    title <- lines[2]  # The title is on the second line
    return(str_trim(title))
  }

  extract_main_text <- function(text) {
    body_start <- str_locate(text, "Body")[, 1]
    classification_start <- str_locate(text, "Classification")[, 1]
    end_document_start <- str_locate(text, "End of Document")[, 1]
    main_text_end <- min(c(na.omit(c(classification_start, end_document_start))), na.rm = TRUE)
    
    if (!is.na(body_start) && !is.na(main_text_end)) {
      main_text <- str_sub(text, body_start + 5, main_text_end - 1)  # +5 to skip "Body"
    } else {
      main_text <- text
    }
    return(str_trim(main_text))
  }

  extract_newspaper <- function(text) {
    newspaper_pattern <- "(?<=Byline:)\\s*[^\\n]+"
    newspaper <- str_extract(text, newspaper_pattern)
    return(str_trim(newspaper))
  }

  extract_length <- function(text) {
    length_pattern <- "(?<=Length:)\\s*\\d+ words"
    length <- str_extract(text, length_pattern)
    return(str_trim(length))
  }

  extract_language <- function(text) {
    language_pattern <- "(?<=Language:)\\s*\\w+"
    language <- str_extract(text, language_pattern)
    return(language)
  }

  # Apply extraction functions on the combined text from all pages
  date <- extract_date(full_text)
  title <- extract_title(full_text)
  main_text <- extract_main_text(full_text)
  newspaper <- extract_newspaper(full_text)
  length <- extract_length(full_text)
  language <- extract_language(full_text)

  # Create a data frame for this PDF
  pdf_df <- data.frame(
    date = date,
    title = title,
    content = main_text,
    source_name = newspaper,
    language = language,
    length = length,
    stringsAsFactors = FALSE
  )

  return(pdf_df)
}


```

## Try on one pdf 

```{r}
pdf_1 <- extract_pdf_info("~/Desktop/MEN Project/2023_el_tiempo/Advertencia de EE. UU. a Venezuela _ 'sanciones volverán si no hay progreso pronto'.PDF")

print(pdf_1)
```


## Loading El Tiempo corpus data

```{r}
# Define PDF directory
el_tiempo_pdf_directory <- "~/Desktop/MEN Project/2023_el_tiempo/"

# Get a list of all PDF files in the directory
el_tiempo_pdf_files <- dir_ls(el_tiempo_pdf_directory, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_tiempo_pdf_df <- map_df(el_tiempo_pdf_files, extract_pdf_info)

# Display the combined data frame
print(el_tiempo_pdf_df)
```

```{r}
# Define PDF directory
el_espectador_pdf_directory <- "~/Desktop/MEN Project/2023_el_espectador/"

# Get a list of all PDF files in the directory
el_espectador_pdf_files <- dir_ls(el_espectador_pdf_directory, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_espectador_pdf_df <- map_df(el_espectador_pdf_files, extract_pdf_info)

# Display the combined data frame
print(el_espectador_pdf_df)
```


### Text pre-processing

```{r}
# Define a text pre-processing function
stop_words <- stopwords("en")  # Define stop words once

preprocess_text <- function(text) {
  text <- tolower(text)  # Convert to lowercase
  text <- str_replace_all(text, "[^[:alpha:][:space:]]", " ")  # Remove punctuation/special characters
  text <- str_replace_all(text, "\\s+", " ")  # Replace multiple spaces with a single space
  text <- str_trim(text)  # Trim leading and trailing spaces
  tokens <- unlist(strsplit(text, "\\s+"))  # Split text into words
  tokens <- tokens[!tokens %in% stop_words]  # Remove English stop words
  tokens <- lemmatize_words(tokens)  # Lemmatize the words
  paste(tokens, collapse = " ")  # Combine tokens back into a single string
}

# Apply preprocessing to each article’s content
el_tiempo_pdf_df <- el_tiempo_pdf_df %>%
  mutate(processed_content = map_chr(content, preprocess_text))

# View the processed data
head(el_tiempo_pdf_df)

```

### Tokenization

```{r}
# Step 1: Tokenize and Preprocess Text for TF-IDF
tidy_text <- el_tiempo_pdf_df %>%
  unnest_tokens(word, processed_content) %>%  # Tokenize the `processed_content` column
  filter(!word %in% stop_words)  # Remove stop words
```

### Calculating TF-IDF

```{r}
# Step 2: Calculate TF-IDF
tf_idf <- tidy_text %>%
  count(title, word, sort = TRUE) %>%  # Count occurrences of each word per document
  bind_tf_idf(word, title, n)  # Calculate tf-idf for each term in each document

# Step 3: Display Top Terms by TF-IDF
top_tf_idf <- tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 10) %>%  # Select top 10 terms per document
  ungroup()

# Step 4: Visualize Top TF-IDF Terms
ggplot(top_tf_idf, aes(x = reorder_within(word, tf_idf, title), y = tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ title, scales = "free_y") +
  scale_x_reordered() +
  labs(x = "Words", y = "TF-IDF") +
  coord_flip()

```
