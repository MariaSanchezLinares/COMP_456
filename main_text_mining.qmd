---
title: "main_text_mining"
format: html
editor: visual
---

```{r, message = FALSE}
# Load Necessary Libraries
library(dplyr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(lubridate)      # For date manipulation functions
library(tidytext)       # For tokenization
library(pdftools) # For PDF text extraction
library(stringr)    # For string manipulation functions, including str_extract()
library(SnowballC)  # For the wordStem function
library(ggplot2)
library(textstem)
library(pdftools)
library(stringr)
library(purrr)
library(fs)  # For file handling
library(textstem)
library(text2vec)
library(data.table)
```

# Data Collection

## Sample PDF
```{r, message = FALSE}
# maria's path
pdf_text_data <- pdf_text("../MEN Project/2020_el_tiempo/Alias Ariel, el poder oculto del Eln que opera en Venezuela.PDF")



print(pdf_text_data[1]) #For page 1
```

Now try this:
## Clean Sample PDF
```{r, message = FALSE}
cat(pdf_text_data[1]) #For page 1
```

## Extracting function
```{r, message = FALSE}

# Function to extract relevant information from a single PDF
extract_pdf_info <- function(pdf_file) {
  # Extract text from all pages of the PDF and concatenate
  pdf_text_data <- pdf_text(pdf_file)
  full_text <- paste(pdf_text_data, collapse = "\n")  # Combine all pages

  # Define extraction functions
  extract_date <- function(text) {
    date_pattern <- "(?<=Load-Date:)\\s*\\w+ \\d{1,2}, \\d{4}"
    date <- str_extract(text, date_pattern)
    return(date)
  }
  
  extract_title <- function(text) {
    lines <- str_split(text, "\n")[[1]]
    title <- lines[2]  # The title is on the second line
    return(str_trim(title))
  }

  extract_main_text <- function(text) {
    body_start <- str_locate(text, "Body")[, 1]
    classification_start <- str_locate(text, "Classification")[, 1]
    end_document_start <- str_locate(text, "End of Document")[, 1]
    main_text_end <- min(c(na.omit(c(classification_start, end_document_start))), na.rm = TRUE)
    
    if (!is.na(body_start) && !is.na(main_text_end)) {
      main_text <- str_sub(text, body_start + 5, main_text_end - 1)  # +5 to skip "Body"
    } else {
      main_text <- text
    }
    return(str_trim(main_text))
  }

  extract_newspaper <- function(text) {
    newspaper_pattern <- "(?<=Byline:)\\s*[^\\n]+"
    newspaper <- str_extract(text, newspaper_pattern)
    return(str_trim(newspaper))
  }

  extract_length <- function(text) {
    length_pattern <- "(?<=Length:)\\s*\\d+ words"
    length <- str_extract(text, length_pattern)
    return(str_trim(length))
  }

  extract_language <- function(text) {
    language_pattern <- "(?<=Language:)\\s*\\w+"
    language <- str_extract(text, language_pattern)
    return(language)
  }

  # Apply extraction functions on the combined text from all pages
  date <- extract_date(full_text)
  title <- extract_title(full_text)
  main_text <- extract_main_text(full_text)
  newspaper <- extract_newspaper(full_text)
  length <- extract_length(full_text)
  language <- extract_language(full_text)

  # Create a data frame for this PDF
  pdf_df <- data.frame(
    date = date,
    title = title,
    content = main_text,
    source_name = newspaper,
    language = language,
    length = length,
    stringsAsFactors = FALSE
  )
  return(pdf_df)
}

```

## Try on one pdf
```{r, message = FALSE}
pdf_1 <- extract_pdf_info("../MEN Project/2023_el_tiempo/Advertencia de EE. UU. a Venezuela _ 'sanciones volverán si no hay progreso pronto'.PDF")

# print(pdf_1)
```

## Loading PDF Datsets 

### El Tiempo 2023
```{r, message = FALSE}
# Define PDF directory
el_tiempo_pdf_directory <- "./2023_el_tiempo/"

# Get a list of all PDF files in the directory
el_tiempo_pdf_files <- dir_ls(el_tiempo_pdf_directory, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_tiempo_pdf_df_2023 <- map_df(el_tiempo_pdf_files, extract_pdf_info)

```

### El Espectador 2023
```{r, message = FALSE}
# Define PDF directory
el_espectador_pdf_directory <- "./2023_el_espectador/"

# Get a list of all PDF files in the directory
el_espectador_pdf_files <- dir_ls(el_espectador_pdf_directory, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_espectador_pdf_df_2023<- map_df(el_espectador_pdf_files, extract_pdf_info)

```

### El Tiempo 2017
```{r, message = FALSE}
# Define PDF directory
el_tiempo_pdf_directory_2017 <- "./2017_el_tiempo/"

# Get a list of all PDF files in the directory
el_tiempo_pdf_files_2017 <- dir_ls(el_tiempo_pdf_directory_2017, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_tiempo_pdf_df_2017 <- map_df(el_tiempo_pdf_files_2017, extract_pdf_info)

```

### El Espectador 2017
```{r, message = FALSE}
# Define PDF directory
el_espectador_pdf_directory_2017 <- "./2017_el_espectador/"

# Get a list of all PDF files in the directory
el_espectador_pdf_files_2017 <- dir_ls(el_espectador_pdf_directory_2017, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_espectador_pdf_df_2017 <- map_df(el_espectador_pdf_files_2017, extract_pdf_info)

```

### El Espectador 2020
```{r, message = FALSE}
# Define PDF directory
el_espectador_pdf_directory_2020 <- "./2020_el_espectador/"

# Get a list of all PDF files in the directory
el_espectador_pdf_files_2020 <- dir_ls(el_espectador_pdf_directory_2020, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_espectador_pdf_df_2020 <- map_df(el_espectador_pdf_files_2020, extract_pdf_info)

```

### El Tiempo 2020
```{r, message = FALSE}
# Define PDF directory
el_tiempo_pdf_directory_2020 <- "./2020_el_tiempo/"

# Get a list of all PDF files in the directory
el_tiempo_pdf_files_2020 <- dir_ls(el_tiempo_pdf_directory_2020, glob = "*.PDF")

# Use purrr to apply the extraction function across all PDF files and combine results into one data frame
el_tiempo_pdf_df_2020 <- map_df(el_tiempo_pdf_files_2020, extract_pdf_info)

```


### Append Articles
```{r}
y2017_articles <- rbind(el_espectador_pdf_df_2017, el_tiempo_pdf_df_2017)

y2020_articles <- rbind(el_tiempo_pdf_df_2020, el_espectador_pdf_df_2020)

y2023_articles <- rbind(el_espectador_pdf_df_2023, el_tiempo_pdf_df_2023)

y2017_articles
y2020_articles
y2023_articles

# create list of datasets fir later
all_articles <- list(y2017_articles, y2020_articles, y2023_articles)

```

# Text Processing 

## Pre-Processing
```{r, message=FALSE}
# y2023_articles <- y2023_articles %>%
#   mutate(date = mdy(date),
#          year = year(date),
#          month = month(date),
#          day = day(date))

# Defining stop words 
stop_words <- c(stopwords("es"), "las", "los", "que", "y", "han", "uno", "dos", "tres", "cuatro", "cinco", "seis", "siete", "ocho", "nueve", "diez", "enero", "febrero", "marzo", "abril", "mayo", "junio", "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre") 

# Defining text processing function
preprocess_text <- function(data, text_column) {
  data <- data %>%
    mutate(processed_text = sapply(!!sym(text_column), function(text) {
      # Preprocess the text
      text <- tolower(text)  # Convert to lowercase
      text <- str_replace_all(text, "[^[:alpha:][:space:]]", " ")  # Remove punctuation
      text <- str_replace_all(text, "\\s+", " ")  # Replace multiple spaces with a single space
      text <- str_trim(text)  # Trim leading and trailing spaces
      tokens <- unlist(strsplit(text, "\\s+"))  # Split text into words
      tokens <- tokens[!tokens %in% stop_words]  # Remove stop words
      tokens <- lemmatize_words(tokens, lang = "es")  # Lemmatize words (in Spanish)
      return(paste(tokens, collapse = " "))  # Combine tokens back into a single string
    }))
  
  return(data)  # Return the dataset with the new column
}

# Apply function on all years 
processed_dts <- lapply(all_articles, preprocess_text, text_column = "content")

# Re-assign datasets names
y2017_articles <- as.data.frame(processed_dts[1])
y2020_articles <- as.data.frame(processed_dts[2]) 
y2023_articles <- as.data.frame(processed_dts[3])

```

### Tokenization
```{r}
# Tokenize and Preprocess Text 
tidy_2023_dt <- y2023_articles %>%
  unnest_tokens(word, processed_text) %>%  # Tokenize the `processed_text` column
  filter(!word %in% stop_words)  # Remove stop words 

tidy_2020_dt <- y2020_articles %>%
  unnest_tokens(word, processed_text) %>%  
  filter(!word %in% stop_words)  

tidy_2017_dt <- y2017_articles %>%
  unnest_tokens(word, processed_text) %>%  
  filter(!word %in% stop_words)  

# Aggregate word counts across all documents
aggregate_2023_dt <- tidy_2023_dt %>%
  count(word, sort = TRUE)

aggregate_2020_dt <- tidy_2020_dt %>%
  count(word, sort = TRUE)

aggregate_2017_dt <- tidy_2017_dt %>%
  count(word, sort = TRUE)

# View the total word counts
print(aggregate_2023_dt)

```

# TF-IDF Matrix
```{r}
# # Function to calculate TF-IDF for a given dataset
# calculate_tfidf <- function(tidy_data) {
#   
#   # Create a document-term matrix (DTM) using the tokens
#   dtm <- tidy_data %>%
#     count(document = row_number(), word) %>%
#     cast_dtm(document, word, n)
#   
#   # Compute the TF-IDF values
#   tfidf <- weightTfIdf(dtm)
#   
#   return(tfidf)
# }
# 
# # Calculate TF-IDF for each dataset
# tfidf_2017 <- calculate_tfidf(tidy_2017_dt)
# tfidf_2020 <- calculate_tfidf(tidy_2020_dt)
# tfidf_2023 <- calculate_tfidf(tidy_2023_dt)
# 
# # Convert the result to a matrix and view
# tfidf_2017_matrix <- as.matrix(tfidf_2017)
# tfidf_2020_matrix <- as.matrix(tfidf_2020)
# tfidf_2023_matrix <- as.matrix(tfidf_2023)
# 
# # Optionally, convert to data frames for easier inspection
# tfidf_2017_df <- as.data.frame(tfidf_2017_matrix)
# tfidf_2020_df <- as.data.frame(tfidf_2020_matrix)
# tfidf_2023_df <- as.data.frame(tfidf_2023_matrix)
# 
# # View the results
# head(tfidf_2017_df)
# head(tfidf_2020_df)
# head(tfidf_2023_df)

```


### TF-IDF Table

```{r}
# Calculate TF-IDF per document
tf_idf_2017 <- tidy_2017_dt %>%
  count(title, word, sort = TRUE) %>%  # Count occurrences of each word per document
  bind_tf_idf(word, title, n) %>%  # Calculate tf-idf for each term in each document
  arrange(desc(tf_idf))

print(tf_idf_2017)
```


